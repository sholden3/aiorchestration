# AI Development Assistant - Comprehensive System Documentation

**Status**: Development Prototype with Comprehensive Planning Complete  
**Version**: 2.0 - Full Documentation Architecture & Multi-Phase Plan  
**Last Updated**: 2025-01-27 - Orchestrated Implementation Framework Active  
**Infrastructure**: Jest ‚úÖ Operational | Pytest ‚úÖ Functional  
**Fix Status**: C1-C3 (Attempted) ‚ö†Ô∏è | H1-H3 (Attempted) ‚ö†Ô∏è  
**Test Results**: IPC Service 58% pass | Terminal Service 12% pass

---

## üìä CURRENT SYSTEM STATE (VALIDATED BY TESTS)

### Test Infrastructure Status
- **Jest Configuration**: ‚úÖ Operational after fixing projects array issue
- **Enhanced Mocks**: ‚úÖ v2 with realistic backend simulation and v1 compatibility
- **Test Discovery**: ‚úÖ Finding and executing all test files
- **Implementation Bugs**: üî¥ Tests revealing service implementation issues

### Discovered Issues (From Today's Session)
1. **IPC Service**: Inconsistent error handling (throw vs null return)
2. **Mock Pattern Mismatch**: Service expected channel methods, mock provided invoke
3. **Window Global Issue**: Jest's window vs global.window inconsistency
4. **NgZone Dependency**: Services require NgZone but tests don't always provide it
5. **Test Design Flaws**: Some tests expect wrong behavior (null instead of errors)

## üéØ PROJECT REALITY CHECK

### What This System Actually Is
**AI Development Platform Prototype** - Desktop application for managing AI agents with intelligent caching, real-time monitoring, and terminal integration. Core infrastructure is production-ready, but AI functionality is currently simulated.

### What Works (Production Ready)
- ‚úÖ **FastAPI Backend**: HTTP server with CORS, lifecycle management, error handling
- ‚úÖ **Intelligent Caching**: Two-tier cache system with 90% hit rate metrics
- ‚úÖ **WebSocket Broadcasting**: Real-time updates and metrics streaming  
- ‚úÖ **Database Integration**: PostgreSQL with connection pooling and fallback to mock
- ‚úÖ **Electron Desktop App**: Cross-platform wrapper with secure IPC
- ‚úÖ **Angular Frontend**: Modern component architecture with Material Design

### What's Simulated (Development Phase)
- ‚ö†Ô∏è **AI Agent Execution**: Returns hard-coded responses instead of real AI interaction
- ‚ö†Ô∏è **Terminal Integration**: PTY sessions tracked but not connected to real processes
- ‚ö†Ô∏è **Claude Integration**: Simulation mode when Claude CLI unavailable

---

## üîÑ DEVELOPMENT GOVERNANCE PROTOCOL

### Mandatory Session Management
Every development session MUST follow this protocol:

#### Session Start (REQUIRED)
```bash
# Before any development work
echo "=== SESSION INITIALIZATION ==="
./validate-session-start.sh
echo "Core Architects: Alex Novak & Dr. Sarah Chen active"
echo "Specialist Pool: Available per PERSONAS.md"
echo "Decision Log: DECISIONS.md ready for updates"
echo "Ready for orchestrated development"
```

#### Session End (REQUIRED)  
```bash
# Before committing any changes
echo "=== SESSION VALIDATION ==="
./validate-task-completion.sh
echo "Both architects must approve before commit"
```

#### Task Completion Verification
Every task requires both architects' explicit sign-off:
- **Sarah**: "Does this pass the Three Questions framework?" (What breaks first? How do we know? What's Plan B?)
- **Alex**: "Does this pass the 3 AM Test?" (Debuggable under pressure? Integration points documented? Cleanup verified?)

### Standardized Prompt Protocols
Use these templates for all development requests:

#### Task Request Template
```
[ORCHESTRATED TASK REQUEST]
Architects: Request both Alex Novak and Dr. Sarah Chen
Task: [specific description]
Requirements:
- Sarah: Analyze failure modes and backend implications  
- Alex: Evaluate frontend integration and process coordination
- Both: Cross-validate assumptions and provide verification
Success Criteria: [specific, measurable outcomes]
```

#### Emergency Fix Template
```
[EMERGENCY FIX REQUEST]
CRITICAL ISSUE DETECTED
Severity: [CRITICAL/HIGH/MEDIUM]
Immediate Actions Needed:
- Sarah: Identify blast radius and containment
- Alex: Determine process coordination impact  
- Both: Provide emergency mitigation steps
```

### Mandatory Archival Rules (ENFORCED)
Every file replacement MUST follow these archival procedures:

#### **Pre-Archival Requirements**
1. **Approval**: Both Alex and Sarah must approve before archiving
2. **Documentation**: Create README.md in archive directory explaining:
   - Why the file is being replaced
   - Performance comparison (old vs new)
   - Rollback procedure
   - Dependencies affected

#### **Archive Structure**
```
archive/
‚îú‚îÄ‚îÄ test_infrastructure/     # Test-related files
‚îú‚îÄ‚îÄ backend_components/      # Backend service files
‚îú‚îÄ‚îÄ frontend_components/     # Frontend/Angular files
‚îî‚îÄ‚îÄ configuration/          # Config files
```

#### **Naming Convention**
```
YYYY-MM-DD_original-filename_v{version}_{reason}.ext
Example: 2025-08-26_test-setup-electron_v1_baseline.ts
```

#### **Rollback Procedure**
1. Locate archived file in appropriate directory
2. Copy back to original location
3. Run validation tests
4. Document rollback reason

**Enforcement**: No file replacements without archival. Violations require architecture review.

---

## üìö DOCUMENTATION NAVIGATION

### Master Documentation Index
**‚û°Ô∏è See [DOCUMENTATION_INDEX.md](./DOCUMENTATION_INDEX.md) for complete project map**

The Documentation Index provides:
- Complete file and folder mapping with descriptions
- Quick navigation to all documentation, code, and processes
- Coverage metrics and status tracking
- Priority document references
- Emergency runbook locations

### Key Documentation References
- **Implementation Roadmap**: [MASTER_IMPLEMENTATION_PLAN.md](./docs/MASTER_IMPLEMENTATION_PLAN.md) - 6-week plan
- **Testing Strategy**: [Test Implementation Plan](./docs/processes/test-implementation-orchestration-plan.md)
- **Framework Guide**: [PERSONAS.md](./PERSONAS.md) - All persona definitions
- **Decision Log**: [DECISIONS.md](./DECISIONS.md) - Binding specialist decisions
- **Project Map**: [DOCUMENTATION_INDEX.md](./DOCUMENTATION_INDEX.md) - Complete navigation

---

## üë• ORCHESTRATED DEVELOPMENT TEAM

### üé≠ Dynamic Persona Model (Core + Specialists)

**Model**: Two permanent core architects + on-demand specialist pool  
**Documentation**: See [PERSONAS.md](./PERSONAS.md) for full persona definitions  
**Decision Tracking**: See [DECISIONS.md](./DECISIONS.md) for specialist decisions  
**Framework Status**: All 10 specialists integrated and operational  

### Core Architects (Always Present)

#### üîß Alex Novak - Senior Electron/Angular Architect
**Specialization**: Desktop application architecture, terminal integration, frontend performance  
**Focus Areas**:
- Electron process coordination and IPC security
- Angular component architecture and memory management  
- PTY terminal integration and process lifecycle management
- Frontend defensive programming and error boundaries

**3 AM Rule**: "If I get paged at 3 AM to debug this system, do I have enough information to fix it without calling anyone else?"

#### üõ°Ô∏è Dr. Sarah Chen - Senior Backend/Systems Architect  
**Specialization**: Python backend systems, caching strategies, WebSocket architecture  
**Focus Areas**:
- Cache failure mode analysis and circuit breaker implementation
- WebSocket connection management and resource limits
- Database connection pooling and graceful degradation
- Backend defensive programming and system resilience

**Three Questions Framework**: Always asks "What breaks first?", "How do we know?", "What's Plan B?"

### Specialist Pool (Invoked as Needed)

Specialists are brought in for domain-specific expertise and must:
- Document all decisions in [DECISIONS.md](./DECISIONS.md)
- Provide inline code documentation for their requirements
- Exit explicitly after their contribution

**Available Specialists**: Defined per project needs in [PERSONAS.md](./PERSONAS.md)

### ü§ù Orchestration Approach
- **Core Continuity**: Alex & Sarah maintain context across all sessions
- **Specialist Expertise**: Domain experts invoked for specific needs
- **Documentation Enforcement**: Every specialist decision must be documented
- **Cross-Validation**: Core architects validate specialist integration

---

## üö® CRITICAL ISSUES - ACTUAL STATUS FROM TESTING

### üî• **SEVERITY: CRITICAL** (Implementation Status)

#### **C1: Memory Leak in Terminal Service** ‚ö†Ô∏è PARTIALLY FIXED
**Current State**: Terminal Manager Service created, but NgZone injection failing  
**Test Results**: 2/17 tests passing (12% pass rate)  
**Remaining Issues**: 
- NgZone dependency injection problems
- IPC listener cleanup not fully validated
- Memory leak prevention mechanism untested
**Next Steps**: Fix NgZone injection, validate cleanup actually works

#### **C2: Cache Disk I/O Failure Cascade** ‚ùì UNTESTED
**Current State**: Fix implemented in backend, no tests available  
**Test Results**: No frontend tests for cache interactions  
**Remaining Issues**: Cannot validate if fix actually works without tests  
**Next Steps**: Create integration tests for cache failure scenarios

#### **C3: Process Coordination Configuration Error** ‚úÖ LIKELY FIXED
**Current State**: Port configuration aligned in documentation  
**Test Results**: Not directly testable via unit tests  
**Validation**: Manual testing required for process coordination  
**Next Steps**: Create integration test for Electron-Backend communication

### ‚ö†Ô∏è **SEVERITY: HIGH** (Implementation Status)

#### **H1: WebSocket Connection Resource Exhaustion** ‚ùì UNTESTED
**Current State**: WebSocket manager exists but limits not validated  
**Test Results**: No tests for connection limits or cleanup  
**Remaining Issues**: Cannot confirm resource limits enforced  
**Next Steps**: Add WebSocket stress tests with connection limits

#### **H2: IPC Error Boundary** ‚ö†Ô∏è PARTIALLY FIXED  
**Current State**: Service implemented with circuit breaker  
**Test Results**: 7/12 tests passing (58% pass rate)  
**Issues Fixed Today**:
- Invoke pattern mismatch resolved
- Consistent error handling implemented
- Window.electronAPI availability fixed
**Remaining Issues**:
- Timeout tests failing
- Circuit breaker state tests failing
- Metrics tracking incomplete
**Next Steps**: Fix timeout handling and circuit breaker state management

#### **H3: Database Initialization Race Condition** ‚ùì UNTESTED
**Current State**: Backend initialization order adjusted  
**Test Results**: No frontend tests validate initialization sequence  
**Remaining Issues**: Race condition prevention unverified  
**Next Steps**: Add startup sequence integration tests

---

## üìÅ ORCHESTRATED FIX ORGANIZATION

### Fix Documentation Structure
```
docs/fixes/
‚îú‚îÄ‚îÄ critical/
‚îÇ   ‚îú‚îÄ‚îÄ C1-terminal-service-memory-leak.md
‚îÇ   ‚îú‚îÄ‚îÄ C2-cache-disk-failure-cascade.md
‚îÇ   ‚îî‚îÄ‚îÄ C3-process-coordination-config.md
‚îú‚îÄ‚îÄ high/
‚îÇ   ‚îú‚îÄ‚îÄ H1-websocket-resource-exhaustion.md
‚îÇ   ‚îú‚îÄ‚îÄ H2-ipc-error-boundaries.md
‚îÇ   ‚îî‚îÄ‚îÄ H3-database-race-condition.md
‚îú‚îÄ‚îÄ medium/
‚îÇ   ‚îú‚îÄ‚îÄ M1-angular-material-bundle-optimization.md
‚îÇ   ‚îî‚îÄ‚îÄ M2-cache-architecture-consolidation.md
‚îî‚îÄ‚îÄ fixes-implementation-plan.md
```

### Fix Implementation Priority
1. **Week 1**: Critical fixes (C1-C3) - System stability
2. **Week 2**: High priority fixes (H1-H3) - Resource management  
3. **Week 3**: Medium priority fixes - Performance optimization
4. **Week 4**: Integration testing and validation

---

## üèóÔ∏è SYSTEM ARCHITECTURE (ACTUAL STATE)

### Backend Components (Python FastAPI)
```
ai-assistant/backend/
‚îú‚îÄ‚îÄ main.py                    # FastAPI app with proper lifecycle
‚îú‚îÄ‚îÄ cache_manager.py           # Two-tier cache (hot/warm) - WORKING
‚îú‚îÄ‚îÄ websocket_manager.py       # Real-time broadcasting - NEEDS LIMITS  
‚îú‚îÄ‚îÄ agent_terminal_manager.py  # Agent simulation - MOCK ONLY
‚îú‚îÄ‚îÄ database_manager.py        # PostgreSQL with mock fallback - WORKING
‚îú‚îÄ‚îÄ persona_manager.py         # Three-persona routing - WORKING
‚îî‚îÄ‚îÄ config.py                  # Configuration management - WORKING
```

### Frontend Components (Angular 17 + Electron)
```
ai-assistant/src/
‚îú‚îÄ‚îÄ app/components/
‚îÇ   ‚îú‚îÄ‚îÄ agent-manager/         # Agent UI - displays mock data
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/             # Metrics dashboard - WORKING
‚îÇ   ‚îî‚îÄ‚îÄ terminal/              # Terminal interface - not connected to PTY
‚îú‚îÄ‚îÄ app/services/
‚îÇ   ‚îú‚îÄ‚îÄ terminal.service.ts    # IPC listeners - MEMORY LEAK
‚îÇ   ‚îú‚îÄ‚îÄ websocket.service.ts   # Real-time updates - WORKING
‚îÇ   ‚îî‚îÄ‚îÄ orchestration.service.ts # Backend integration - WORKING
‚îî‚îÄ‚îÄ electron/
    ‚îú‚îÄ‚îÄ main.js                # Process management - PORT MISMATCH
    ‚îú‚îÄ‚îÄ preload.js             # Secure IPC bridge - WORKING
    ‚îî‚îÄ‚îÄ pty-manager.js         # Terminal management - not fully integrated
```

---

## üéì LESSONS LEARNED FROM TEST IMPLEMENTATION

### Key Discoveries (Both Architects Validated)

1. **Tests Reveal Truth**: Documentation claimed fixes were complete, tests proved otherwise
2. **Implementation != Working**: Having code in place doesn't mean it works correctly
3. **Mock Fidelity Matters**: Mismatch between mock and service patterns caused failures
4. **Test Design Quality**: Some tests were testing wrong behavior (expecting null instead of errors)
5. **Global Scope Complexity**: Jest's window vs global.window caused hard-to-debug issues

### What Went Right
- ‚úÖ Jest configuration eventually worked after removing projects array
- ‚úÖ Enhanced mocks with backward compatibility maintained test stability
- ‚úÖ Cross-architect challenges revealed fundamental issues
- ‚úÖ Incremental fixes improved understanding

### What Went Wrong
- ‚ùå Assumed fixes were complete without test validation
- ‚ùå Initial Jest configuration used unsupported API patterns
- ‚ùå Service implementation had inconsistent error handling
- ‚ùå Tests weren't independent (shared global state)

### Process Improvements Adopted
1. **No Fix Claims Without Tests**: All fixes must have passing tests as proof
2. **Mandatory Cross-Challenge**: Each architect must challenge the other's assumptions
3. **Test First, Then Fix**: Write/run tests to understand the problem before fixing
4. **Document Patterns**: Capture architectural patterns as we discover them

---

## üìö DISCOVERED PATTERNS & MANDATORY RULES

### Architectural Patterns (Both Architects Validated)

#### **Pattern 1: Consistent Error Handling**
**Problem**: Mixed throw/null returns make service unpredictable  
**Solution**: Services must EITHER always throw OR always return null, never both  
**Implementation**:
```typescript
// CORRECT: Consistent throwing pattern
if (!fallbackValue) throw error;
return fallbackValue;

// WRONG: Mixed behavior
if (!fallbackValue) return null;  // Sometimes null
if (circuitOpen) throw error;     // Sometimes throw
```
**Enforced By**: Both architects in code review

#### **Pattern 2: Mock-Service Contract Alignment**
**Problem**: Service expects channel methods, mock provides invoke pattern  
**Solution**: Services must support both patterns for compatibility  
**Implementation**:
```typescript
// Support both patterns
if (typeof electronAPI.invoke === 'function') {
  return electronAPI.invoke(channel, data);
} else if (typeof electronAPI[channel] === 'function') {
  return electronAPI[channel](data);
}
```
**Enforced By**: Alex (frontend integration)

#### **Pattern 3: Test Environment Consistency**
**Problem**: window vs global.window inconsistency in Jest  
**Solution**: Always set both in test setup  
**Implementation**:
```typescript
// In test setup files
(global as any).window = windowMock;
if (typeof window !== 'undefined') {
  Object.assign(window, windowMock);
}
```
**Enforced By**: Sarah (test infrastructure)

### Business Rules (Updated from Testing Discoveries)

1. **Test-Driven Validation**: All fix claims must be validated by passing tests
2. **No Silent Failures**: Services must either succeed, use fallback, or throw - never silently fail
3. **Backward Compatibility**: New mocks must maintain compatibility with existing tests
4. **Cross-Domain Impact**: Frontend changes must consider backend timing, backend changes must consider frontend UX

### Coding Rules (Enforced in Review)

#### **Frontend Rules (Alex's Domain)**
- All IPC calls must be wrapped in error boundaries
- Component services must handle cleanup in ngOnDestroy
- Mock compatibility must be tested for both invoke and channel patterns
- Window globals must be set consistently in tests

#### **Backend Rules (Sarah's Domain)**
- Circuit breakers must have consistent open/closed behavior
- Resource limits must be enforced with clear error messages
- Timeout values must account for maintenance windows
- All async operations must have proper cleanup

### Testing Rules (Mandatory)

1. **Test Independence**: Each test must control its own environment
2. **Error Expectations**: Tests must validate specific error types, not just "any error"
3. **Mock State Reset**: Always reset mock state between tests
4. **Dependency Injection**: Use TestBed.inject, not direct instantiation

### Definition of "Done" for Fixes

A fix is only considered DONE when ALL of the following are met:

#### **Code Complete**
- [ ] Implementation matches fix specification
- [ ] Error handling follows consistent patterns
- [ ] Resource cleanup verified
- [ ] No magic numbers or hardcoded values

#### **Testing Complete**
- [ ] Unit tests pass with >90% coverage of fix
- [ ] Integration tests validate cross-system behavior
- [ ] Edge cases explicitly tested
- [ ] Performance benchmarks met

#### **Review Complete**
- [ ] Alex approved (frontend/integration aspects)
- [ ] Sarah approved (backend/system aspects)
- [ ] No outstanding review comments
- [ ] Patterns documented if new ones discovered

#### **Validation Complete**
- [ ] Manual testing in development environment
- [ ] No memory leaks detected
- [ ] No performance regressions
- [ ] Monitoring/logging adequate

**Enforcement**: No fix moves from "Partial" to "Complete" without all checkboxes checked.

---

## üóìÔ∏è PHASE BREAKDOWN - REMAINING WORK

### Phase 0: Current State Assessment ‚úÖ COMPLETED
- Jest infrastructure operational
- Tests revealing implementation bugs
- 58% IPC tests passing, 12% Terminal tests passing

### Phase 1: Fix Implementation Bugs (In Progress)
**Timeline**: 2-3 days  
**Alex's Focus**: 
- Fix remaining 5 IPC test failures
- Resolve NgZone dependency injection in Terminal service
- Ensure consistent error handling patterns

**Sarah's Focus**:
- Validate backend mock behavior matches production
- Review circuit breaker state transitions
- Ensure resource cleanup patterns

**Success Criteria**: 
- IPC tests: >90% pass rate
- Terminal tests: >80% pass rate
- No memory leaks detected

### Phase 2: Complete Test Coverage
**Timeline**: 3-4 days  
**Objectives**:
- Add tests for all critical paths
- Implement integration tests for cross-system boundaries
- Add performance benchmarks

**Success Criteria**:
- Frontend coverage: >80%
- Backend coverage: >85%
- All integration points tested

### Phase 3: Fix Remaining High/Medium Priority Issues
**Timeline**: 1 week  
**Focus Areas**:
- H1: WebSocket connection limits
- H2: Complete IPC error boundaries
- H3: Database initialization race condition
- M1-M2: Performance optimizations

**Success Criteria**:
- All high priority issues resolved
- Performance benchmarks met
- No race conditions detected

### Phase 4: Production Hardening
**Timeline**: 1 week  
**Activities**:
- Stress testing with realistic load
- Memory leak detection under load
- Error recovery validation
- Documentation updates

**Success Criteria**:
- 8-hour stability test passes
- Memory usage stable under load
- All error scenarios handled gracefully

### Phase 5: Deployment Preparation
**Timeline**: 3-4 days  
**Final Steps**:
- Production configuration
- Deployment scripts
- Monitoring setup
- Runbook validation

**Success Criteria**:
- Clean deployment to staging
- All monitors reporting correctly
- Runbooks tested by someone unfamiliar with system

---

## üöÄ ORCHESTRATED DEVELOPMENT WORKFLOW

### Pre-Development Validation (Both Architects)
```bash
# Sarah's Backend Validation
cd ai-assistant/backend
python -m pytest -v                    # Verify tests pass
python -c "import main; print('OK')"   # Check imports work

# Alex's Frontend Validation  
cd ai-assistant
npm run build                          # Verify Angular compiles
npm run electron:dev                   # Check Electron launches
```

### Cross-Validation Requirements
Every change must pass both architects' validation:

**Sarah's Checklist**:
- [ ] What breaks first? (Failure mode analysis)
- [ ] How do we know? (Monitoring and observability)
- [ ] What's Plan B? (Fallback and recovery)
- [ ] Circuit breakers implemented where needed
- [ ] Resource limits enforced

**Alex's Checklist**:
- [ ] Passes 3 AM test (debuggable under pressure)
- [ ] IPC error boundaries in place
- [ ] Memory cleanup verified  
- [ ] Process coordination tested
- [ ] Angular change detection optimized

### Implementation Standards
```typescript
// Alex's Defensive Pattern Template
class DefensiveService {
  async safeOperation<T>(operation: () => Promise<T>): Promise<T> {
    try {
      return await operation();
    } catch (error) {
      this.logError(error);
      throw new ApplicationError('Operation failed', { originalError: error });
    }
  }
}

// Sarah's Circuit Breaker Pattern Template  
class CircuitBreaker {
  async execute<T>(operation: () => Promise<T>): Promise<T> {
    if (this.isOpen()) {
      throw new CircuitBreakerOpenError('Service unavailable');
    }
    
    try {
      const result = await operation();
      this.recordSuccess();
      return result;
    } catch (error) {
      this.recordFailure();
      throw error;
    }
  }
}
```

---

## üìù CODE DOCUMENTATION REQUIREMENTS (MANDATORY)

### Source File Documentation Standards
Every source file MUST include comprehensive documentation that eliminates the need to search external documentation for understanding:

#### File Header Requirements (All Files)
```typescript
/**
 * @fileoverview [Concise description of file purpose and functionality]
 * @author [Persona Name] v[Version] - [Date]
 * @architecture [Component it belongs to - Frontend/Backend/Integration]
 * @responsibility [Primary responsibility within system architecture]
 * @dependencies [Key dependencies and why needed]
 * @integration_points [How this connects to other system components]
 * @testing_strategy [What aspects require testing and why]
 * @governance [Which governance frameworks apply - Sarah's/Alex's/Specialist]
 * 
 * Business Logic Summary:
 * - [Primary business rules implemented]
 * - [Key validation logic]
 * - [Error handling approach]
 * 
 * Architecture Integration:
 * - [How this fits in system overview]
 * - [Security boundaries involved]
 * - [Performance considerations]
 */
```

#### Class Documentation Requirements
```typescript
/**
 * @class [ClassName]
 * @description [What this class does and why it exists]
 * @architecture_role [How this fits in the overall system design]
 * @business_logic [Key business rules this class enforces]
 * @failure_modes [What can break and how it's handled - Sarah's Framework]
 * @debugging_info [Key information for 3AM debugging - Alex's Framework]
 * 
 * Defensive Programming Patterns:
 * - [Circuit breakers implemented]
 * - [Input validation approach]
 * - [Resource limits enforced]
 * 
 * Integration Boundaries:
 * - [External systems accessed]
 * - [Security controls applied]
 * - [Error propagation strategy]
 */
class ExampleService {
```

#### Method Documentation Requirements
```typescript
/**
 * @method [methodName]
 * @description [What this method does in business context]
 * @business_rule [Business logic implemented by this method]
 * @validation [Input validation performed and why]
 * @side_effects [Any state changes or external calls made]
 * @error_handling [How errors are detected and handled]
 * @performance [Expected performance characteristics]
 * @testing_requirements [What scenarios must be tested]
 * 
 * @param {type} paramName - [Business meaning and validation rules]
 * @returns {type} [What is returned and under what conditions]
 * @throws {ErrorType} [When and why this error is thrown]
 * 
 * Architecture Notes:
 * - [How this method fits in component design]
 * - [Security considerations]
 * - [Monitoring/observability hooks]
 * 
 * Sarah's Framework Check:
 * - What breaks first: [Primary failure mode]
 * - How we know: [Detection mechanism]
 * - Plan B: [Fallback strategy]
 */
async exampleMethod(param: Type): Promise<ReturnType> {
```

#### Business Logic Documentation
All business logic implementations MUST include:

```typescript
// BUSINESS RULE: [Clear statement of business rule being implemented]
// VALIDATION: [What validation is performed and why]
// ERROR HANDLING: [How business rule violations are handled]
// AUDIT TRAIL: [What gets logged for compliance/debugging]

if (businessCondition) {
    // BUSINESS LOGIC: [Explain the business reasoning]
    // SARAH'S FRAMEWORK: This handles failure mode X with fallback Y
    // ALEX'S 3AM TEST: Debug info includes correlation ID and rule name
    
    return processBusinessRule(data);
}
```

#### Integration Point Documentation
```typescript
/**
 * INTEGRATION POINT: [System boundary being crossed]
 * SECURITY BOUNDARY: [Security controls at this point]
 * ERROR PROPAGATION: [How errors are handled across boundary]
 * MONITORING: [What metrics are collected here]
 * GOVERNANCE: [AI governance checks if applicable]
 */
async callExternalSystem(request: RequestType): Promise<ResponseType> {
    // CIRCUIT BREAKER: Protect against external system failures
    // RATE LIMITING: Prevent abuse of external resources  
    // AUDIT LOGGING: Record all boundary crossings for compliance
}
```

### Documentation Validation Rules

#### Pre-Commit Documentation Checks (MANDATORY)
```bash
#!/bin/bash
# validate-code-documentation.sh

# Check for mandatory file headers
find . -name "*.ts" -o -name "*.py" | while read file; do
    if ! grep -q "@fileoverview" "$file"; then
        echo "ERROR: Missing @fileoverview in $file"
        exit 1
    fi
    
    if ! grep -q "@business_logic" "$file"; then
        echo "ERROR: Missing business logic documentation in $file" 
        exit 1
    fi
done

# Check for class documentation
grep -r "^class\|^export class" --include="*.ts" | while read -r line; do
    file=$(echo "$line" | cut -d: -f1)
    if ! grep -A 10 -B 5 "$line" "$file" | grep -q "@architecture_role"; then
        echo "ERROR: Missing architecture documentation for class in $file"
        exit 1
    fi
done

# Check for method documentation on public methods
# (Implementation would scan for public methods and verify documentation)

echo "‚úÖ All code documentation requirements met"
```

#### Documentation Completeness Score
Every source file receives a documentation score:
```
File Documentation Score = (
    File Header (25 points) +
    Class Documentation (25 points) + 
    Method Documentation (25 points) +
    Business Logic Comments (25 points)
) / 100

Minimum Score Required: 90/100
```

#### Integration with Architecture Documentation
Code documentation must reference architecture documentation:
```typescript
/**
 * @architecture_reference docs/architecture/component-design/frontend-architecture.md#ipc-security
 * @security_reference docs/architecture/component-design/security-boundaries.md#ipc-boundary
 * @api_contract_reference docs/architecture/data-flow/api-contracts.md#agent-api
 */
```

### Specialist Persona Documentation Requirements

#### When Creating Business Logic (Requires Specialist)
```typescript
/**
 * SPECIALIST DECISION: [Specialist Name] v[Version] - [Date]
 * DECISION REFERENCE: DECISIONS.md#[decision-id]
 * RATIONALE: [Why this approach was chosen by specialist]
 * CONSTRAINTS: [What cannot be changed without specialist approval]
 * VALIDATION: [How to verify correct implementation]
 */
```

#### AI Governance Documentation
```typescript
/**
 * AI GOVERNANCE: This component integrates with AI governance system
 * GOVERNANCE_HOOKS: [Which hooks are triggered - pre/post agent spawn, etc.]
 * PERSONA_ORCHESTRATION: [Which personas are activated and when]
 * VALIDATION_PIPELINE: [Which of the 4 gates this passes through]
 * COST_CONTROLS: [Token limits and budget enforcement]
 */
```

## üîí QUALITY GATES & ENFORCEMENT

### Pre-Commit Requirements (MANDATORY)
All changes must pass these automated checks:

```bash
# Critical quality gates - must pass for any commit
./validate-task-completion.sh

# Code documentation validation (MANDATORY)
./validate-code-documentation.sh

# Project structure validation  
./validate-project-structure.sh

# Cross-system integration validation
npm run test:integration-critical
```

### Testing Coverage Requirements

#### Minimum Test Coverage Standards
- **Backend**: 85% line coverage, 90% for critical modules
- **Frontend**: 80% line coverage, 85% for services
- **Integration**: 100% coverage of cross-system boundaries
- **Failure Scenarios**: All identified failure modes must have tests

#### Required Test Categories
```bash
# Backend (Sarah's Domain)
python -m pytest tests/ --cov=. --cov-fail-under=85
python -m pytest tests/integration/ -v  
python -m pytest tests/test_failure_modes.py -v

# Frontend (Alex's Domain)
ng test --watch=false --code-coverage
ng e2e --suite=critical-path
npm run test:memory-leaks

# Cross-System (Both Architects)
npm run test:electron-backend-coordination
npm run test:websocket-integration
npm run test:failure-cascades
```

### Documentation Currency Requirements
- **CLAUDE.md**: Updated within 7 days of any major change
- **Fix Documentation**: Created within 24 hours of issue identification  
- **Architecture Docs**: Updated within 48 hours of system changes
- **Runbooks**: Verified monthly and after any production incident

### Archival Rules (Mandatory)
**All file replacements must follow this protocol:**

1. **Pre-Archival Approval**: Both Alex and Sarah must approve before any file is archived
2. **Archive Structure**:
   ```
   archive/
   ‚îú‚îÄ‚îÄ test_infrastructure/     # Test-related files
   ‚îú‚îÄ‚îÄ backend_components/       # Backend service files  
   ‚îú‚îÄ‚îÄ frontend_components/      # Frontend/Angular files
   ‚îî‚îÄ‚îÄ documentation/           # Superseded documentation
   ```

3. **Archive Naming Convention**: 
   `YYYY-MM-DD_original-filename_v{version}_issue{issue-number}.ext`
   Example: `2025-08-26_test-setup-electron_v1_issueH2.ts`

4. **Required Metadata** (in accompanying README.md):
   - **Approved By**: Both architects' sign-off
   - **Related Issue**: C1-C3, H1-H3, or feature reference
   - **Reason for Replacement**: Specific bugs or enhancements
   - **Performance Metrics**: Before/after if applicable
   - **Rollback Instructions**: How to restore if needed
   - **Dependencies**: What else might be affected

5. **Rollback Strategy**:
   - Archived files must remain functional
   - Test suite must validate archived version compatibility
   - Maximum 5-minute rollback time requirement

### Project Structure Enforcement
Required directory structure (validated automatically):
```
docs/
‚îú‚îÄ‚îÄ fixes/              # All issues and solutions
‚îú‚îÄ‚îÄ architecture/       # System design documentation
‚îú‚îÄ‚îÄ processes/          # Development processes
‚îî‚îÄ‚îÄ runbooks/          # 3 AM debugging procedures

tests/
‚îú‚îÄ‚îÄ integration/        # Cross-system tests
‚îú‚îÄ‚îÄ performance/        # Performance validation
‚îî‚îÄ‚îÄ failure-scenarios/  # Failure mode tests
```

---

## üß™ TESTING STRATEGY (ORCHESTRATED)

### Backend Testing (Sarah's Domain)
```bash
# Unit Tests - Business Logic Validation
python -m pytest backend/test_cache_manager.py -v
python -m pytest backend/test_websocket_manager.py -v
python -m pytest backend/test_orchestrator.py -v

# Integration Tests - System Boundaries
python -m pytest backend/test_database_integration.py -v
python -m pytest backend/test_api_endpoints.py -v

# Performance Tests - Resource Limits
python -m pytest backend/test_cache_performance.py -v
python -m pytest backend/test_websocket_load.py -v
```

### Frontend Testing (Alex's Domain)
```bash
# Unit Tests - Component Logic
ng test --watch=false --browsers=ChromeHeadless

# Integration Tests - Service Communication  
ng e2e

# Memory Leak Tests - Resource Cleanup
npm run test:memory-leaks

# IPC Tests - Electron Communication
npm run test:ipc-boundaries
```

### Cross-Integration Testing (Both Architects)
```bash
# End-to-End System Tests
npm run test:e2e:full-system

# Failure Mode Tests
npm run test:failure-scenarios

# Performance Integration Tests  
npm run test:performance:integrated
```

---

## üìä MONITORING & OBSERVABILITY

### Sarah's Backend Metrics
- **Cache Performance**: Hit rate, latency percentiles, eviction rate
- **WebSocket Health**: Connection count, message throughput, error rate
- **Database Status**: Connection pool utilization, query performance
- **System Resources**: Memory usage, CPU utilization, disk I/O

### Alex's Frontend Metrics  
- **Electron Process Health**: Main/renderer memory usage, IPC latency
- **Angular Performance**: Component lifecycle, change detection cycles
- **Terminal Operations**: PTY session count, command response times
- **User Interface**: Render performance, interaction responsiveness

### Correlation Tracking
All logs include correlation IDs for tracing requests across:
- HTTP API calls
- WebSocket messages  
- IPC communications
- Database operations

---

## üîß QUICK START (ACTUAL WORKING INSTRUCTIONS)

### Prerequisites
- Node.js 18+
- Python 3.10+  
- PostgreSQL 14+ (optional - falls back to mock)
- Git (for terminal emulation on Windows)

### Development Setup
```bash
# 1. Backend Setup (Sarah's domain)
cd ai-assistant/backend
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt

# 2. Frontend Setup (Alex's domain)  
cd ai-assistant
npm install
npm rebuild node-pty  # Important for terminal support

# 3. Configuration Fix (Critical - addresses C3)
# Edit electron/main.js: Change backendPort from 8001 to 8000
```

### Running the System
```bash
# Terminal 1: Start Backend (port 8000)
cd ai-assistant/backend  
python main.py

# Terminal 2: Start Frontend
cd ai-assistant
npm run electron:dev
```

### Health Verification
```bash
# Backend Health Check
curl http://localhost:8000/health

# WebSocket Connection Test
# Open browser dev tools in Electron app, check WebSocket connection

# Database Status (if PostgreSQL available)
curl http://localhost:8000/db/status
```

### Comprehensive Governance Framework
For complete governance procedures, see: `docs/processes/governance-framework.md`

**Key Framework Components:**
- **Session Management**: Mandatory start/end validation protocols
- **Quality Gates**: Automated testing and validation requirements  
- **Documentation Standards**: Currency requirements and templates
- **Emergency Procedures**: Critical issue response protocols
- **Process Enforcement**: Git hooks and automated validation

**Implementation Commands:**
```bash
# Install governance framework
curl -O docs/processes/governance-framework.md
chmod +x validate-*.sh

# Daily workflow integration
alias start-work='./validate-session-start.sh'
alias finish-work='./validate-task-completion.sh'  
alias check-ready='./validate-project-structure.sh'
```

**Enforcement Level**: MANDATORY - No exceptions for production commits

---

## üõ†Ô∏è QUICK COMMAND REFERENCE

### Testing Commands
```bash
# Frontend Testing (Alex's Domain)
cd ai-assistant
npm test                              # Run all tests
npm test -- --testPathPattern=ipc     # Run specific test file pattern
npm test -- --testNamePattern="should handle"  # Run specific test by name
npm test -- --coverage                # Generate coverage report
npm test -- --passWithNoTests         # Run even if no tests found

# Backend Testing (Sarah's Domain)  
cd ai-assistant/backend
python -m pytest                      # Run all backend tests
python -m pytest -v                   # Verbose output
python -m pytest --cov=.              # With coverage
python -m pytest -k "test_cache"      # Run specific tests

# Integration Testing
npm run test:integration              # Cross-system tests
```

### Development Commands
```bash
# Start Backend
cd ai-assistant/backend && python main.py

# Start Frontend
cd ai-assistant && npm run electron:dev

# Build Production
npm run build && npm run electron:build

# Check for Issues
npm run lint
npm run typecheck
```

### Debugging Commands
```bash
# Find magic numbers
grep -rn "\b[0-9]\{3,\}\b" src/ --include="*.ts"

# Check memory usage
ps -o pid,vsz,rss,comm -p $(pgrep -f "electron")

# Find IPC listeners
grep -r "addEventListener\|on\|once" src/ --include="*.ts" | grep -i ipc

# Check for cleanup
grep -r "removeEventListener\|off\|removeAllListeners" src/ --include="*.ts"
```

---

## üÜò EMERGENCY DEBUGGING (3 AM PROCEDURES)

### Backend Down
```bash
# Check Python process
ps aux | grep python | grep main.py

# Check port conflicts
netstat -an | grep :8000

# Check logs
tail -f ai-assistant/backend/logs/app.log

# Emergency restart
cd ai-assistant/backend && python main.py
```

### Frontend Unresponsive
```bash
# Check Electron processes
ps aux | grep electron

# Check IPC connectivity
# In Electron DevTools: window.electronAPI

# Memory usage check
# DevTools ‚Üí Memory tab ‚Üí Heap Snapshots

# Emergency restart
npm run electron:dev
```

### Memory Leak Investigation
```bash
# Backend memory check
ps -o pid,vsz,rss,comm -p $(pgrep -f "python.*main.py")

# Frontend memory check  
# Electron DevTools ‚Üí Performance ‚Üí Memory

# Specific leak patterns to check
grep -r "addEventListener\|removeEventListener" src/
grep -r "setInterval\|setTimeout" src/
```

---

## üìà SUCCESS METRICS

### Technical Performance
- **System Stability**: 99.9% uptime during development sessions
- **Cache Efficiency**: >90% hit rate, <10ms hot cache access
- **Memory Management**: No memory leaks during 8-hour sessions
- **Response Times**: <500ms API responses, <100ms IPC operations

### Development Velocity  
- **Bug Fix Time**: Critical issues resolved within 24 hours
- **Feature Integration**: Cross-layer changes validated within 2 hours
- **Documentation Currency**: All fixes documented within same day
- **Test Coverage**: >90% backend coverage, >80% frontend coverage

---

## üó∫Ô∏è ROADMAP

### Phase 1: Stabilization (Current)
- [ ] Fix all critical issues (C1-C3)
- [ ] Implement defensive patterns throughout  
- [ ] Add comprehensive error boundaries
- [ ] Establish monitoring and alerting

### Phase 2: Real AI Integration  
- [ ] Replace simulated agent responses with actual AI calls
- [ ] Implement real PTY terminal integration
- [ ] Add Claude CLI integration for production use
- [ ] Create agent capability management

### Phase 3: Production Readiness
- [ ] Performance optimization and load testing
- [ ] Security audit and hardening  
- [ ] Deployment automation
- [ ] User documentation and training

---

**Orchestrated by Alex Novak & Dr. Sarah Chen**  
*"The best architecture is code that works perfectly, fails gracefully, documents itself completely, and teaches the next developer exactly why every decision was made‚Äîespecially when they're debugging it during a production crisis."*

---

*This documentation reflects the actual state of the system as of January 2025. All claims are evidence-based and cross-validated by both architects.*